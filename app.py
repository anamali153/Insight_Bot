# -*- coding: utf-8 -*-
"""AI_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-TVC2PKWUNuenFuJVBPMUs6vhkOLZi-s
"""

#pip install python-dotenv torch transformers bitsandbytes huggingface_hub accelerate optimum

import os
import torch
import transformers
from dotenv import load_dotenv
from huggingface_hub import login
from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TextStreamer

"""Conversation Memory Trim Function"""

def trim_history(messages, max_length=3000):
    # Combine all message content
    text = "".join([m["content"] for m in messages])

    # If still short â†’ no need to trim
    if len(text) < max_length:
        return messages

    # Otherwise create a conversation summary
    summary_prompt = [
        {"role": "system", "content": "Summarize the following conversation in 5 sentences."},
        {"role": "user", "content": text}
    ]

    summary_response = client.chat.completions.create(
        model="meta-llama/Llama-3.2-1B-Instruct",
        messages=summary_prompt,
        max_tokens=200
    )

    summary = summary_response.choices[0].message["content"]

    # Return trimmed history
    return [
        {"role": "system", "content": "Summary so far: " + summary},
        *messages[-5:]
    ]

"""AI Personalities"""

PERSONALITIES = {
    "default": "You are a normal helpful assistant.",
    "Teacher": "You explain concepts with simple examples.",
    "Friendly": "You speak like a warm, supportive friend.",
    "Programmer": "You answer like a senior software engineer.",
    "Strict": "Give short, factual answers only.",
    "Child-Friendly Tutor": "Explain everything like you are teaching a 7-year-old child."
}

""" SAFETY GUARD"""

def safety_guard(user_text):
    blocked_words = ["kill", "harm", "suicide", "bomb"]

    for w in blocked_words:
        if w in user_text.lower():
            return "â— I cannot help with harmful or dangerous requests."

    return None

"""TOOLS (Calculator + Wikipedia)"""

import wikipedia
import math

def tool_calculator(expr):
    try:
        return str(eval(expr))
    except:
        return "Invalid mathematical expression."

def tool_wiki(query):
    try:
        return wikipedia.summary(query, sentences=3)
    except:
        return "No Wikipedia results found."

#!pip install wikipedia

#load_dotenv()  # if you store token in .env
#login(token=os.getenv("HF_TOKEN"))

#from huggingface_hub import whoami
#print(whoami())

device = "cuda" if torch.cuda.is_available() else "cpu"

LLAMA = "meta-llama/Llama-3.2-1B-Instruct"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    LLAMA,
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(LLAMA)
tokenizer.pad_token = tokenizer.eos_token

def extract_text(decoded: str) -> str:
    """Clean special tokens from the generated text."""

    if "<|start_header_id|>assistant<|end_header_id|>" in decoded:
        assistant_part = decoded.split("<|start_header_id|>assistant<|end_header_id|>")[-1]
        assistant_reply = assistant_part.split("<|eot_id|>")[0].strip()
    else:
        assistant_reply = decoded.strip()  # fallback if no special tokens

    return assistant_reply

def generate_response(
    messages: str,
    output_tokens: int = 256,
    tokenizer: transformers.tokenization_utils_fast = tokenizer,
    model: transformers.models = model,
    device: str = device
) -> str:
    # Tokenization
    inputs = tokenizer.apply_chat_template(
        messages,
        return_tensors="pt",
        padding=True
    ).to(device)

    # Model Inference
    outputs = model.generate(
        inputs,
        max_new_tokens=output_tokens,
        pad_token_id=tokenizer.pad_token_id
    )

    # Post-processing
    response_text = extract_text(tokenizer.decode(outputs[0]))

    return response_text

"""new Generate response"""

def generate_response(system_prompt, personality, user_prompt, temperature, history):
    # 1. Safety check
    safe = safety_guard(user_prompt)
    if safe:
        return safe, history

    # 2. Tools
    if user_prompt.lower().startswith("calc:"):
        result = tool_calculator(user_prompt[5:])
        history.append((user_prompt, result))
        return result, history

    if user_prompt.lower().startswith("wiki:"):
        result = tool_wiki(user_prompt[5:])
        history.append((user_prompt, result))
        return result, history

    # -------------------------
    # Build messages for model
    # -------------------------
    messages = []

    # System prompt + personality
    full_system_prompt = PERSONALITIES[personality] + "\n" + system_prompt
    messages.append({"role": "system", "content": full_system_prompt})

    # History
    for u, a in history:
        messages.append({"role": "user", "content": u})
        messages.append({"role": "assistant", "content": a})

    # New user message
    messages.append({"role": "user", "content": user_prompt})

    # Trim conversation
    messages = trim_history(messages)

    # -------------------------
    # Convert messages to tokens
    # -------------------------
    inputs = tokenizer.apply_chat_template(
        messages,
        return_tensors="pt",
        padding=True
    ).to(device)

    # -------------------------
    # Generate using local Llama
    # -------------------------
    output_ids = model.generate(
        inputs,
        max_new_tokens=300,
        temperature=temperature,
        pad_token_id=tokenizer.eos_token_id
    )

    decoded = tokenizer.decode(output_ids[0])
    reply = extract_text(decoded)

    # Save to history
    history.append((user_prompt, reply))

    return reply, history

system_prompt = "You are a helpful assistant."
personality = "default"   # whatever key exists in PERSONALITIES
temperature = 0.7
history = []

messages = []
prompt = ""

while prompt != "quit":
    prompt = input("User: ")

    if prompt == "quit":
        break

    # Call your actual generate_response() function with correct parameters
    generated_text, history = generate_response(
        system_prompt=system_prompt,
        personality=personality,
        user_prompt=prompt,
        temperature=temperature,
        history=history
    )

    print(f"\nAssistant: {generated_text}\n")



"""Gradio App"""

import gradio as gr

with gr.Blocks(title="Advanced LLM Chatbot") as demo:

    gr.Markdown("# ðŸ¤– Advanced AI Assistant")

    personality = gr.Dropdown(
        list(PERSONALITIES.keys()),
        value="Friendly",
        label="Choose AI Personality"
    )

    system_prompt = gr.Textbox(
        label="Custom System Instructions (optional)",
        placeholder="e.g., 'Explain everything with examples'"
    )

    user_input = gr.Textbox(label="Your Message")

    temperature = gr.Slider(0.0, 1.0, value=0.4, label="Temperature")

    output = gr.Textbox(label="Response")
    history = gr.State([])

    btn = gr.Button("Send")

    btn.click(
        generate_response,
        inputs=[system_prompt, personality, user_input, temperature, history],
        outputs=[output, history]   # âœ” correct
    )

demo.launch()